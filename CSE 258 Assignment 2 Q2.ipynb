{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83543c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from collections import defaultdict\n",
    "# Initialize dictionaries\n",
    "\n",
    "\n",
    "# Read and shuffle the data\n",
    "with open('100k_a.csv', 'r') as file:\n",
    "    data = list(csv.reader(file))\n",
    "    random.shuffle(data)\n",
    "\n",
    "# Splitting the data: 80% train, 20% validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa47bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_interactions=defaultdict(int)\n",
    "user_interactions=defaultdict(int)\n",
    "for row in data:\n",
    "    username=row[0]\n",
    "    streamer=row[2]\n",
    "    streamer_interactions[streamer]+=1\n",
    "    user_interactions[username]+=1\n",
    "\n",
    "small_streamer=set()\n",
    "small_user=set()\n",
    "\n",
    "for streamer in streamer_interactions.keys():\n",
    "    if streamer_interactions[streamer]<3:\n",
    "        small_streamer.add(streamer)\n",
    "        \n",
    "        \n",
    "for username in user_interactions.keys():\n",
    "    if user_interactions[username]<5:\n",
    "        small_user.add(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606de92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=[]\n",
    "\n",
    "for row in data:\n",
    "    username=row[0]\n",
    "    streamer=row[2]\n",
    "    if username in small_user or streamer in small_streamer:\n",
    "        continue\n",
    "    else:\n",
    "        new_data.append(row)\n",
    "    \n",
    "    \n",
    "    \n",
    "split_index = int(0.8 * len(new_data))\n",
    "train_data = new_data[:split_index]\n",
    "validation_data = new_data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ca5b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928857\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e388591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single data row\n",
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "streamer_viewers = defaultdict(int)\n",
    "all_streamers=set()\n",
    "# Process train data\n",
    "for row in train_data:\n",
    "    username = row[0]\n",
    "    streamer = row[2]\n",
    "    usersPerItem[streamer].add(username)\n",
    "    itemsPerUser[username].add(streamer)\n",
    "    streamer_viewers[streamer]+=1\n",
    "    all_streamers.add(streamer)\n",
    "\n",
    "all_streamers=list(all_streamers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ca146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "dupe_count=0\n",
    "balanced_valid=[]\n",
    "for row in validation_data:\n",
    "    username=row[0]\n",
    "    streamer=row[2]\n",
    "    while True:\n",
    "        random_streamer = random.choice(all_streamers)\n",
    "        balanced_valid.append([username,streamer,1])\n",
    "        if random_streamer not in itemsPerUser[username]:\n",
    "            balanced_valid.append([username, random_streamer,0])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bccf63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value: 41.180494433378115\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "mean_value = statistics.mean(streamer_viewers.values())\n",
    "mean_value+=5\n",
    "print(\"Mean Value:\", mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad11169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy is 0.8521209938537007\n",
      "false positives 115903 false_negatives 57376\n"
     ]
    }
   ],
   "source": [
    "corr=0\n",
    "false_positives=0\n",
    "false_negatives=0\n",
    "for row in balanced_valid:\n",
    "    username=row[0]\n",
    "    streamer=row[1]\n",
    "    if streamer_viewers[streamer]>mean_value:\n",
    "        pred=1\n",
    "    else:\n",
    "        pred=0\n",
    "        \n",
    "    if(row[2]==pred):\n",
    "        \n",
    "        corr+=1\n",
    "    else:\n",
    "        if(row[2]==1):\n",
    "            false_positives+=1\n",
    "        else:\n",
    "            false_negatives+=1\n",
    "            \n",
    "print('baseline accuracy is',corr/len(balanced_valid))\n",
    "print('false positives', false_positives, 'false_negatives',false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb51081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171762\n"
     ]
    }
   ],
   "source": [
    "print(len(balanced_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ccece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7eadf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a4c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8301668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394895be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fe8bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 18:42:26.687318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +                            tf.nn.l2_loss(self.gammaU) +                            tf.nn.l2_loss(self.gammaI))\n",
    "    \n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e04b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_games(threshold):\n",
    "    #game_count is a global dictionary\n",
    "    mostPopular = [(streamer_viewers[x], x) for x in streamer_viewers]\n",
    "    mostPopular.sort(reverse=True)\n",
    "\n",
    "    temp=0\n",
    "    popular_games = set()\n",
    "    for times_played, game in mostPopular:\n",
    "        temp += times_played\n",
    "        popular_games.add(game)\n",
    "        if temp > (len(train_data) * threshold):   # see if i am over the threshold\n",
    "            return popular_games\n",
    "\n",
    "\n",
    "userIDs={}\n",
    "itemIDs={}\n",
    "class BPR_Predictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.interactions = []\n",
    "\n",
    "    def fit(self, dataset, threshold=0.5, iters=200):\n",
    "        self.popular_games = get_popular_games(threshold)\n",
    "        #basically also from workbook all the way down to training step just renamed variables for me\n",
    "        for i in train_data:\n",
    "            user=i[0]\n",
    "            game=i[2]\n",
    "            if not user in userIDs: userIDs[user] = len(userIDs)\n",
    "            if not game in itemIDs: itemIDs[game] = len(itemIDs)\n",
    "            self.interactions.append((user,game,1))\n",
    "                \n",
    "        itemsPerUser = defaultdict(list)\n",
    "        usersPerItem = defaultdict(list)\n",
    "        items_keys=list(itemIDs.keys())\n",
    "        for user, game, _ in self.interactions:\n",
    "            itemsPerUser[user].append(game)\n",
    "            usersPerItem[game].append(user)\n",
    "        #below from workbook, changed names of variables for clarity\n",
    "        def training_step(model, interactions):\n",
    "            batch_size = 50000\n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_user, batch_game, batch_item = [], [], []\n",
    "                for _ in range(batch_size):\n",
    "                    #pick a positive and negative sample\n",
    "                    user,game,_ = random.choice(interactions) \n",
    "                    item_here = random.choice(items_keys) \n",
    "                    while item_here in itemsPerUser[user]:\n",
    "                        item_here = random.choice(items_keys)\n",
    "                    batch_user.append(userIDs[user])\n",
    "                    batch_game.append(itemIDs[game])\n",
    "                    batch_item.append(itemIDs[item_here])\n",
    "\n",
    "                loss = model(batch_user,batch_game,batch_item)\n",
    "                loss += model.reg()\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients((grad, var) for\n",
    "                                    (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                                    if grad is not None)\n",
    "            return loss.numpy()\n",
    "        #below up to predict also from workbook\n",
    "        optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "        self.modelBPR = BPRbatch(K=6, lamb= 0.00001)\n",
    "\n",
    "        for i in range(iters):\n",
    "            obj = training_step(self.modelBPR, self.interactions)\n",
    "            if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n",
    "    #below also from workbook        \n",
    "    def predict(self, user, game, threshold=0.5):\n",
    "        if user in userIDs and game in itemIDs: # add this because user or item id not in lists we made\n",
    "            pred = self.modelBPR.predict(userIDs[user], itemIDs[game]).numpy()\n",
    "            if(pred>threshold):\n",
    "                return 1\n",
    "            return 0\n",
    "        else:\n",
    "            if(game in self.popular_games):\n",
    "                return 1\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27c1d481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR_Predictor accuracy:  0.8472676191922933 47619 131347\n"
     ]
    }
   ],
   "source": [
    "#model = BPR_Predictor()\n",
    "\n",
    "\n",
    "#model.fit(train_data) #train on whole set lol\n",
    "\n",
    "correct = 0\n",
    "tot=0\n",
    "false_positives=0\n",
    "false_negatives=0\n",
    "for user, game, review in balanced_valid:\n",
    "    tot+=1\n",
    "    pred = model.predict(user, game)\n",
    "    if pred == review:\n",
    "        correct += 1\n",
    "    elif(pred==1 and review==0):\n",
    "        false_positives+=1\n",
    "    else:\n",
    "        false_negatives+=1\n",
    "    #print(f\"BPR_Predictor accuracy: \", correct / tot,false_positives,false_negatives)\n",
    "print(f\"BPR_Predictor accuracy: \", correct / len(balanced_valid),false_positives,false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a390564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
